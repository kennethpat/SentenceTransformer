{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417b6e63-c0d0-4c24-bd19-eb5cd0bbf7d2",
   "metadata": {},
   "source": [
    "# **Task 1: Sentence Transformer Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155c4f1d-05be-4732-8708-1baf4c822282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21299c01-e184-4673-bdf6-e2d393090be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"distilbert-base-uncased\", \n",
    "                 embedding_dim: int = 768, \n",
    "                 pooling_strategy: str = \"mean\", \n",
    "                 max_length: int = 128) -> None:\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.use_projection = None\n",
    "        if embedding_dim != self.transformer.config.hidden_size:\n",
    "            self.use_projection = True\n",
    "        else:\n",
    "            self.use_projection = False\n",
    "\n",
    "        if self.use_projection is True:\n",
    "            self.projection = nn.Linear(self.transformer.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, sentences: list[str]) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        device = next(self.transformer.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.transformer(**inputs)\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if self.pooling_strategy == \"cls\":\n",
    "            pooled = hidden_states[:, 0]\n",
    "        elif self.pooling_strategy == \"max\":\n",
    "            pooled = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            pooled = torch.sum(hidden_states * attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "\n",
    "        if self.use_projection is True:\n",
    "            pooled = self.projection(pooled)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def encode(self, sentences: list[str], batch_size: int = 32, normalize: bool = False) -> np.ndarray:\n",
    "        self.eval()\n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(sentences), batch_size):\n",
    "                batch = sentences[i : i + batch_size]\n",
    "                embeddings = self.forward(batch)\n",
    "                if normalize is True:\n",
    "                    embeddings = nn.functional.normalize(embeddings)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c838be0-7446-4084-b628-525598dd3323",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 embeddings with dimension 384\n",
      "Sample of the first embedding vector:\n",
      "[ 0.01348774  0.02294292 -0.07688586 -0.05261944 -0.04356397  0.05017351\n",
      " -0.05570818 -0.0127305   0.04803646 -0.02740604]\n",
      "Computing similarities between sentences:\n",
      "Similarity between sentence 1 and 2: 0.5727\n",
      "Similarity between sentence 1 and 3: 0.8527\n",
      "Similarity between sentence 1 and 4: 0.8012\n",
      "Similarity between sentence 1 and 5: 0.7978\n",
      "Similarity between sentence 2 and 3: 0.6158\n",
      "Similarity between sentence 2 and 4: 0.5676\n",
      "Similarity between sentence 2 and 5: 0.5876\n",
      "Similarity between sentence 3 and 4: 0.6867\n",
      "Similarity between sentence 3 and 5: 0.8547\n",
      "Similarity between sentence 4 and 5: 0.6446\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    test_model = SentenceTransformer(model_name=\"distilbert-base-uncased\", embedding_dim=384, pooling_strategy=\"mean\", max_length=128)\n",
    "    test_sentences = [\n",
    "            \"This is a simple sentence to encode.\",\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"Sentence transformers are useful for many NLP tasks.\",\n",
    "            \"This sentence is similar to the first one but with different words.\",\n",
    "            \"Machine learning models can process natural language effectively.\"\n",
    "    ]\n",
    "    test_embeddings = test_model.encode(sentences=test_sentences, normalize=True)\n",
    "    print(f\"Generated {test_embeddings.shape[0]} embeddings with dimension {test_embeddings.shape[1]}\")\n",
    "    print(f\"Sample of the first embedding vector:\\n{test_embeddings[0][:10]}\")\n",
    "    print(\"Computing similarities between sentences:\")\n",
    "    for i in range(len(test_sentences)):\n",
    "        for j in range(i + 1, len(test_sentences)):\n",
    "            similarity = np.dot(test_embeddings[i], test_embeddings[j])\n",
    "            print(f\"Similarity between sentence {i + 1} and {j + 1}: {similarity:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979b11b-624e-4ac9-be77-f8377cec9a34",
   "metadata": {},
   "source": [
    "## Description for Task 1\n",
    "### Choosing a suitable large language model\n",
    "I started Task 1 by selecting the suitable architectural model for the transformer. I decided to choose DistilBERT, a distilled version of BERT. BERT itself has been a popular large language model (LLM) for natural language processing (NLP) experiments for many years. Although DistilBERT is smaller, we can use it to achieve a 60% faster training speed while maintaining a 95% performance, as suggested by HuggingFace. By using DistilBERT, we can reduce the computational burden by 40% while retaining 97% of its language understanding capabilities and being 60% faster.\n",
    "\n",
    "### Other design choices\n",
    "For pooling strategies, I implemented three different pooling strategies in the code. They play a significant role no how the meaning of a word inside a sentence is parsed and interpreted. The **mean pooling** strategy works by calculating the mean among all token embeddings, which is the default strategy used by the sentence transformer model. For instance, if we want to get a $2 \\times 2$ sample pool size, then we can calculate the mean for each of the 4 token embeddings and create a downsized pool. The \"CLS\" in **CLS token** stands for \"classification\". It works by going through tokenization at sentence-level classification, which is an essential feature in BERT. The last strategy is **max pooling**, which is similar to the mean pooling aforementioned. But instead of calculating the mean values, it takes the maximum values.\n",
    "\n",
    "Moving on, let's discuss the dimension reduction layer of my model. By default, DistilBERT has a dimensionality of 768 for the encoder layers and the pooler layer. However, I've reduced this number to half (384) in my code by using an optional projection layer. By cutting the embedding dimension in half, I've also reduced the computational burden and storage requirements. Furthermore, it helps to prevent overfitting in downstream tasks, and makes calculations on similarity values faster.\n",
    "\n",
    "Additionally, I've also added an option for normalization on the embeddings, which uses the L2 normalization to calculate the norm in a Euclidean space. The L2 norm is calculated as:\n",
    "\n",
    "$$\\displaystyle \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{k = 1}^n |x_k|^2} = \\sqrt{x_1^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "The norm option ensures that all embeddings have unit length and simplified the calculations of cosine similarity. It also improves peroformance in retrieval tasks. Cosine similarity is calculated as a dot product as:\n",
    "\n",
    "$$\\mathbf{A} \\cdot \\mathbf{B} =\\left\\|\\mathbf{A} \\right\\| \\left\\| \\mathbf{B}\\right\\| \\cos \\theta$$\n",
    "\n",
    "Finally, I implemented batch processing in the `encode` function by passing the parameter `batch_size`. Introducing batch processing to lists and other large size data helps to prevent memory issues associated with large size inputs and improves the customizability of the code for future developers.\n",
    "\n",
    "### References:\n",
    "\n",
    "[DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c350b0-c8d6-447f-a753-a0a5cd4a2264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023cc46e-cd52-4977-8604-edd481148032",
   "metadata": {},
   "source": [
    "# **Task 2: Multi-Task Learning Expansion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c0174a-877a-44ee-8d99-851bc412febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd533b42-3d4f-4f19-939f-52fe214c375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerMultiTask(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"distilbert-base-uncased\",\n",
    "                 embedding_dim: int = 768,\n",
    "                 pooling_strategy: str = \"mean\",\n",
    "                 max_length: int = 128,\n",
    "                 num_sentiment_classes: int = 3) -> None:\n",
    "        super(SentenceTransformerMultiTask, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_length = max_length\n",
    "        self.transformer_dim = self.transformer.config.hidden_size\n",
    "\n",
    "        self.use_projection = None\n",
    "        if embedding_dim != self.transformer_dim:\n",
    "            self.use_projection = True\n",
    "        else:\n",
    "            self.use_projection = False\n",
    "\n",
    "        if self.use_projection is True:\n",
    "            self.projection = nn.Linear(self.transformer_dim, embedding_dim)\n",
    "            self.shared_dim = embedding_dim\n",
    "        else:\n",
    "            self.shared_dim = self.transformer_dim\n",
    "\n",
    "        self.sentiment_classifier = nn.Sequential(nn.Linear(self.shared_dim, 256), nn.ReLU(), nn.Dropout(0.1), nn.Linear(256, num_sentiment_classes))\n",
    "        self.sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "    def _extract_features(self, sentences: list[str]) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        device = next(self.transformer.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = self.transformer(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling_strategy == \"cls\":\n",
    "            pooled = hidden_states[:, 0]\n",
    "        elif self.pooling_strategy == \"max\":\n",
    "            pooled = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            pooled = torch.sum(hidden_states * attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "\n",
    "        if self.use_projection is True:\n",
    "            pooled = self.projection(pooled)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, sentences: list[str], task: Optional[str] = None) -> dict[str, torch.Tensor]:\n",
    "        shared_features = self._extract_features(sentences)\n",
    "        outputs = {}\n",
    "        \n",
    "        if task is None or task == \"embedding\":\n",
    "            outputs[\"embedding\"] = shared_features\n",
    "\n",
    "        if task is None or task == \"sentiment\":\n",
    "            sentiment_logits = self.sentiment_classifier(shared_features)\n",
    "            outputs[\"sentiment\"] = sentiment_logits\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def encode(self, sentences: list[str], batch_size: int = 32, normalize: bool = False) -> np.ndarray:\n",
    "        self.eval()\n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(sentences), batch_size):\n",
    "                batch = sentences[i : i + batch_size]\n",
    "                outputs = self.forward(batch, task=\"embedding\")\n",
    "                embeddings = outputs[\"embedding\"]\n",
    "                if normalize is True:\n",
    "                    embeddings = nn.functional.normalize(embeddings)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "    def predict_sentiment(self, sentences: list[str], batch_size: int = 32) -> dict[str, list[str]]:\n",
    "        self.eval()\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(sentences), batch_size):\n",
    "                batch = sentences[i : i + batch_size]\n",
    "                outputs = self.forward(batch, task=\"sentiment\")\n",
    "                logits = outputs[\"sentiment\"]\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predictions = torch.argmax(probabilities, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.append(probabilities.cpu().numpy())\n",
    "        prediction_labels = [self.sentiment_labels[pred] for pred in all_predictions]\n",
    "        all_probabilities = np.vstack(all_probabilities)\n",
    "        results = {\"labels\": prediction_labels, \"probabilities\": all_probabilities}\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5869949e-f511-4c25-a09b-ae508553507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], labels: list[int], tokenizer, max_length: int = 128) -> None:\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, str]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        item = {\"text\": text, \"label\": label}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9e897b-671d-4c31-b98f-bf4e7ffea2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_model(\n",
    "    model: SentenceTransformerMultiTask, \n",
    "    train_texts: list[str], \n",
    "    train_labels: list[int], \n",
    "    val_texts: Optional[list[str]] = None, \n",
    "    val_labels: Optional[list[int]] = None, \n",
    "    epochs: int = 5, \n",
    "    batch_size: int = 16, \n",
    "    learning_rate: float = 2e-5) -> tuple[SentenceTransformerMultiTask, dict[str, list[float]]]:\n",
    "\n",
    "    assert isinstance(epochs, int), \"'epochs' must be an integer.\"\n",
    "    assert epochs > 0, \"'epochs' must be positive.\"\n",
    "\n",
    "    train_dataset = SentimentDataset(train_texts, train_labels, model.tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    if val_texts is not None and val_labels is not None:\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, model.tokenizer)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "\n",
    "    training_stats = {\n",
    "        \"train_losses\": [],\n",
    "        \"val_losses\": [],\n",
    "        \"val_accuracies\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"===== Epoch {epoch + 1} / {epochs} =====\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            texts = batch[\"text\"]\n",
    "            labels = batch[\"label\"]\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(texts, task=\"sentiment\")\n",
    "            logits = outputs[\"sentiment\"]\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        training_stats[\"train_losses\"].append(avg_train_loss)\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        if val_loader is True:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    texts = batch[\"text\"]\n",
    "                    labels = batch[\"label\"]\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(texts, task=\"sentiment\")\n",
    "                    logits = outputs[\"sentiment\"]\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = correct / total\n",
    "            training_stats[\"val_losses\"].append(avg_val_loss)\n",
    "            training_stats[\"val_accuracies\"].append(val_accuracy)\n",
    "\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\\nAccuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66d5eaf-69e1-46ad-909b-a42df3b610b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING SENTENCE EMBEDDINGS:\n",
      "Generated 5 embeddings with dimension 384\n",
      "Sample of the first embedding vector:\n",
      "[-0.07832588 -0.05834525 -0.02594784 -0.06107131 -0.05892345 -0.03642315\n",
      "  0.00334449 -0.0333345  -0.06209832 -0.01038289]\n",
      "PREDICTING SENTIMENT:\n",
      "Text: This is a simple sentence to encode.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3134, Neutral = 0.3492, Positive = 0.3374\n",
      "\n",
      "Text: The quick brown fox jumps over the lazy dog.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3280, Neutral = 0.3449, Positive = 0.3271\n",
      "\n",
      "Text: I absolutely love this product, it's amazing!\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3333, Neutral = 0.3340, Positive = 0.3327\n",
      "\n",
      "Text: This movie was terrible and a complete waste of time.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3286, Neutral = 0.3436, Positive = 0.3278\n",
      "\n",
      "Text: The service was okay, nothing special but not bad either.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3293, Neutral = 0.3395, Positive = 0.3312\n",
      "\n",
      "TRAINING EXAMPLE (with dummy data):\n",
      "Training the model on sentiment analysis task...\n",
      "===== Epoch 1 / 20 =====\n",
      "Train Loss: 1.0930\n",
      "===== Epoch 2 / 20 =====\n",
      "Train Loss: 1.0731\n",
      "===== Epoch 3 / 20 =====\n",
      "Train Loss: 1.0200\n",
      "===== Epoch 4 / 20 =====\n",
      "Train Loss: 0.9765\n",
      "===== Epoch 5 / 20 =====\n",
      "Train Loss: 0.9094\n",
      "===== Epoch 6 / 20 =====\n",
      "Train Loss: 0.8373\n",
      "===== Epoch 7 / 20 =====\n",
      "Train Loss: 0.7890\n",
      "===== Epoch 8 / 20 =====\n",
      "Train Loss: 0.6959\n",
      "===== Epoch 9 / 20 =====\n",
      "Train Loss: 0.6246\n",
      "===== Epoch 10 / 20 =====\n",
      "Train Loss: 0.5809\n",
      "===== Epoch 11 / 20 =====\n",
      "Train Loss: 0.5192\n",
      "===== Epoch 12 / 20 =====\n",
      "Train Loss: 0.4555\n",
      "===== Epoch 13 / 20 =====\n",
      "Train Loss: 0.4209\n",
      "===== Epoch 14 / 20 =====\n",
      "Train Loss: 0.3558\n",
      "===== Epoch 15 / 20 =====\n",
      "Train Loss: 0.2918\n",
      "===== Epoch 16 / 20 =====\n",
      "Train Loss: 0.2648\n",
      "===== Epoch 17 / 20 =====\n",
      "Train Loss: 0.2414\n",
      "===== Epoch 18 / 20 =====\n",
      "Train Loss: 0.2148\n",
      "===== Epoch 19 / 20 =====\n",
      "Train Loss: 0.1912\n",
      "===== Epoch 20 / 20 =====\n",
      "Train Loss: 0.1768\n",
      "Re-testing sentiment prediction after training:\n",
      "Text: This is a simple sentence to encode.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.1800, Neutral = 0.6258, Positive = 0.1942\n",
      "\n",
      "Text: The quick brown fox jumps over the lazy dog.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.3186, Neutral = 0.3665, Positive = 0.3149\n",
      "\n",
      "Text: I absolutely love this product, it's amazing!\n",
      "Predicted sentiment: positive\n",
      "Class probabilities: Negative = 0.0785, Neutral = 0.0801, Positive = 0.8414\n",
      "\n",
      "Text: This movie was terrible and a complete waste of time.\n",
      "Predicted sentiment: negative\n",
      "Class probabilities: Negative = 0.8016, Neutral = 0.0925, Positive = 0.1058\n",
      "\n",
      "Text: The service was okay, nothing special but not bad either.\n",
      "Predicted sentiment: neutral\n",
      "Class probabilities: Negative = 0.1670, Neutral = 0.6690, Positive = 0.1640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    test_model = SentenceTransformerMultiTask(model_name=\"distilbert-base-uncased\", embedding_dim=384, pooling_strategy=\"mean\", max_length=128, num_sentiment_classes=3)\n",
    "    test_sentences = [\n",
    "        \"This is a simple sentence to encode.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"I absolutely love this product, it's amazing!\",\n",
    "        \"This movie was terrible and a complete waste of time.\",\n",
    "        \"The service was okay, nothing special but not bad either.\"\n",
    "    ]\n",
    "\n",
    "    print(\"GENERATING SENTENCE EMBEDDINGS:\")\n",
    "    test_embeddings = test_model.encode(sentences=test_sentences, normalize=True)\n",
    "    print(f\"Generated {test_embeddings.shape[0]} embeddings with dimension {test_embeddings.shape[1]}\")\n",
    "    print(f\"Sample of the first embedding vector:\\n{test_embeddings[0][:10]}\")\n",
    "    print(\"PREDICTING SENTIMENT:\")\n",
    "    test_sentiment_results = test_model.predict_sentiment(test_sentences)\n",
    "\n",
    "    for i, (text, label) in enumerate(zip(test_sentences, test_sentiment_results[\"labels\"])):\n",
    "        probs = test_sentiment_results[\"probabilities\"][i]\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Predicted sentiment: {label}\")\n",
    "        print(f\"Class probabilities: Negative = {probs[0]:.4f}, Neutral = {probs[1]:.4f}, Positive = {probs[2]:.4f}\\n\")\n",
    "\n",
    "    print(\"TRAINING EXAMPLE (with dummy data):\")\n",
    "    test_train_texts = [\n",
    "        \"I love this product.\",\n",
    "        \"This is terrible.\",\n",
    "        \"It's okay I guess.\",\n",
    "        \"Best purchase ever!\",\n",
    "        \"Complete waste of money.\",\n",
    "        \"It works as expected.\"\n",
    "    ]\n",
    "    test_train_labels = [2, 0, 1, 2, 0, 1]\n",
    "    print(\"Training the model on sentiment analysis task...\")\n",
    "    test_model, test_stats = train_multitask_model(\n",
    "        model=test_model, \n",
    "        train_texts=test_train_texts, \n",
    "        train_labels=test_train_labels, \n",
    "        epochs=20, \n",
    "        batch_size=2)\n",
    "\n",
    "    print(\"Re-testing sentiment prediction after training:\")\n",
    "    test_sentiment_results = test_model.predict_sentiment(test_sentences)\n",
    "    for i, (text, label) in enumerate(zip(test_sentences, test_sentiment_results[\"labels\"])):\n",
    "        probs = test_sentiment_results[\"probabilities\"][i]\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Predicted sentiment: {label}\")\n",
    "        print(f\"Class probabilities: Negative = {probs[0]:.4f}, Neutral = {probs[1]:.4f}, Positive = {probs[2]:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a64486-4097-4825-babc-776fc053caa4",
   "metadata": {},
   "source": [
    "## Description for Task 2\n",
    "I've built task 2 on top of the code from task 1 to handle multi-task learning by implementing sentiment analysis on the embeddings. Sentiement analysis is a concept in NLP that allows us to analyze and classify the emotional states from text or voice inputs. In this case, we are focusing on the text part.\n",
    "\n",
    "The fundamental part of building the multi-task learning expansion is the creation of a **shared representation layer**. It captures the natural language understanding (NLU), while also turned the transformer backbone and pooling layers into shared feature extractors. All the task-specific heads are able to use the shared features. More importantly, this approach explains the principle that learning multiple related tasks simultaneously can improve the quality of the shared representation layer.\n",
    "\n",
    "For each of the two tasks (embeddings and sentiment analysis), components were added to adjust and reinforce their features. As for encoding embeddings in the `encode` function, it now uses the shared representations directly on top of its original functionality. As for the sentiment analysis, a new two-layer neural network (NN, using `torch.nn`) was created with 256 hidden units and ReLU activation. A dropout regularization rate was set to 0.1 to prevent overfitting. The neural network also has an output layer producing logits for 3 sentiment classes (negative, neutral, positive).\n",
    "\n",
    "By adding more features to the model, it now allows users to select which task to execute. This is seen by the `task` parameter in the `forward` function, which takes a string. Additionally, the outputs are stored in a dictionary (variable `outputs`) with keys for each task.\n",
    "\n",
    "Two essential new functions were added to the model. `predict_sentiment` is the function that predicts the sentiment in a given sentence. It's a dedicated inference method for sentiment analysis that returns both predicted labels and class probabilities. It's also capable of performing batch processing for efficient inference on multiple sentences, as well as converting numeric predictions to pre-defined string labels. `train_multitask_model` is the main function for the training process. It sets up the optimization process by using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) in PyTorch and contains the implementation of the training and validation loops. It allows users to input the number of epochs for training and keeps track of the loss and accuracy values.\n",
    "\n",
    "Finally, to support the training process, a `SentimentDataset` class was created by using the PyTorch [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) interface with inheritance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416c7aa-939b-4e21-9e62-de8e552759ce",
   "metadata": {},
   "source": [
    "# **Task 3: Training Considerations**\n",
    "## 1. If the entire network should be frozen.\n",
    "If the entire network is frozen, then we are only left with using the model for inference without any adaptation to new data.\n",
    "\n",
    "### Implications:\n",
    "\n",
    "- Models relies entirely on knowledge from pre-trained weights\n",
    "- Sentiment analysis tasks would be limited by the rate of alignment between the pre-trained model's representation space and the sentiment concepts\n",
    "- The quality of embeddings would remain constant, thus preserving the original semantic relationships\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Very fast execution time during the training phase (since there's no actual training)\n",
    "- No risk of losing pre-trained knowledge\n",
    "- Consistent and predictable behavior across different datasets\n",
    "- Minimum requirements for computational power\n",
    "\n",
    "### We should use this method if:\n",
    "\n",
    "- We have extremely limited computational power\n",
    "- Our domain is almost identical to the pre-trained data\n",
    "- We need absolute consistency in representations\n",
    "\n",
    "## 2. If only the transformer backbone should be frozen.\n",
    "\n",
    "### Implications:\n",
    "\n",
    "- Shared natural language understanding (NLU) remains fixed\n",
    "- Model learns how to interpret the existing representations for specific tasks\n",
    "- Creates a difference between \"knowledge for natural languages\" and \"knowledge for specific tasks\"\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Preserves knowledge related to linguistics in pre-trained model\n",
    "- Significantly reduces execution time during the training process and requirements for computational power\n",
    "- Reduces the risk on overfitting for small datasets\n",
    "\n",
    "### We should use this method if:\n",
    "- We are low on computational power (but more than that stated in 1.)\n",
    "- We have some data on task-specific tasks, but it's not enough to fine-tune a model safely\n",
    "- We want to frequently add new tasks but don't want to retrain all data\n",
    "- We want to quickly adapt to new tasks while maintaining a consistent representation space\n",
    "\n",
    "## 3. If only one of the task-specific heads should be frozen.\n",
    "\n",
    "### Implications:\n",
    "\n",
    "- Model would optimize its representations primarily for the embedding task\n",
    "- Classifier for sentiment analysis needs to adapt to shifting representations as the training elapses\n",
    "- Fixed classifier enforces constraints on the amount which the shared representation can drift\n",
    "\n",
    "### Advantages:\n",
    "- Optimization on backbone particularly for high-quality embeddings\n",
    "- May improve embedding performance\n",
    "- Maintains some level of functionality for sentiment analysis\n",
    "\n",
    "### We should use this method if:\n",
    "\n",
    "- Our main goal is to optimize the quality for embeddings\n",
    "- Sentiment analysis is not our main goal\n",
    "- We have a well-trained sentiment classifier which we want to maintain\n",
    "\n",
    "## 1. The choice of a pre-trained model.\n",
    "\n",
    "First, I would choose a model such as [RoBERTa-base](https://huggingface.co/FacebookAI/roberta-base) or [MPNet-base](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) for the following reasons:\n",
    "\n",
    "- RoBERTa-base could make improvements on BERT since it contains more robust training methodology and larger data for training\n",
    "- MPNet-base combines the strengths of masked language modeling and permuted language modeling, which makes it beneficial in transfer training\n",
    "- Both models have shown their strengths in natural language understanding (NLU)\n",
    "- Both models provide a balance between performance and computational burden\n",
    "\n",
    "## 2. The layers you would freeze/unfreeze.\n",
    "\n",
    "I would use a gradual unfreezing strategy for the transfer learning process. At the beginning, I would freeze the transformer backbone and run training tasks on task-specific heads for less than 10 epochs. Then, I would unfreeze the top layers of the transformer and continue training with a reduced learning rate. Finally, I would gradually unfreeze more layers and train them with smaller learning rates.\n",
    "\n",
    "## 3. The rationale behind these choices.\n",
    "\n",
    "This gradual unfreezing strategy that I intended to use contains the following benefits:\n",
    "\n",
    "- Specialization on layers' position: In a transformer model, layers located at the front would capture more patterns on linguistic characteristics, whereas layers at the back would encode information more on specific tasks. By adopting this gradual unfreezing strategy from top to bottom, we could make the most task-relevant layers adapt first.\n",
    "- Prevention for knowledge loss: The gradual unfreezing strategy could help prevent the model from losing pre-trained knowledge in the middle of the training process.\n",
    "- Increase of efficiency on training data: This strategy also helps to increase the use of limited task-specific data by utilizing the parameters related to adaptation at the beginning.\n",
    "- Balance on multi-tasking: Being able to control the amount of change on shared representations allows us to create a balance in performance among multiple tasks. If one tasks is consuming an excessive amount of computational power, the learning rates can be adjusted to resolve the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26f42a-8fca-4c07-9ded-2d821df5711e",
   "metadata": {},
   "source": [
    "# **Task 4: Training Loop Implementation (BONUS)**\n",
    "## 1. Handling of hypothetical data\n",
    "\n",
    "Compare to the single-task learning model I created in task 1, the multi-task learning model in task 2 is more complex and requires more consideration on handling data. First, it would require data balance between tasks. If we were to train this model, balancing the training signal between tasks if one has significantly more data would be one of the points we need to consider. The current architecture assumes that learning to perform sentiment analysis and generating quality embeddings complement each other and can benefit from shared representations. Lastly, to maintain the quality of the embeddings, we might need to develop a back-translation technique to boost the semantic meaning for emotional words in sentences.\n",
    "\n",
    "## 2. Forward pass\n",
    "\n",
    "The `forward` function I implemented takes a `task` parameter as a string. Depending on the content, it can tell the `forward` function to perform different tasks:\n",
    "\n",
    "- `None`: run all tasks\n",
    "- `\"embedding\"`: generate embeddings only\n",
    "- `\"sentiment\"`: run sentiment analysis only\n",
    "\n",
    "By designing the `forward` function like this, we could increase the overall efficiency by running tasks that are needed by the user, while also enabling joint training where all tasks are running simultaneously.\n",
    "\n",
    "Also, during backpropagation, the gradients would flow through different channels. If only the sentiment analysis task is run, gradients would flow through the sentiment classifier and then into the shared layers; however, if both tasks are run, gradients from both tasks influence the shared representations. By doing so, we create an implicit weighting of tasks based on their loss magnitudes.\n",
    "\n",
    "## 3. Metrics\n",
    "\n",
    "My implementation also tracks metrics separately for each task. In sentiment analysis, we track the values for **loss and accuracy**. Currently, there's no track being tracked when generating embeddings, but if we were to turn the code into a real implementation, we would need to track metrics that are related embeddings as well, including sentiment accuracy, cosine similarity between semantically related sentences, correlation in performance between one task and another, etc. The current training loop for the Multi-Task Learning Expansion focuses primarily on the **classification** of sentences. But in a real-world implementation, we would need to collect the total loss by calculating the summation from sentiment loss and embedding loss.\n",
    "\n",
    "Another feature that I didn't demonstrate is validation. In a real-world machine learning pipeline, validation data needs to be supplied from a reliable source and cannot be fabricated or made up. The validation set serves as a critical independent check on a model's performance and helps to detect issues like overfitting. What makes this even more challenging is the fact that different tasks could reach their respective point of best performance at different time. The current code contains validation metrics for sentiment analysis, but a real-world implementation would need to have validation metrics for all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17645aa0-0723-4f97-b395-7131c93f4fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
